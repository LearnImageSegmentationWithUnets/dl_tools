{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid CNN - CRF semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Written by Dr Daniel Buscombe, Northern Arizona University\n",
    "\n",
    "> Part of a series of notebooks for image recognition and classification using deep convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates a method to use a DCNN retrained on a particular dataset (library of image tiles), and apply it to an input image for semantic segmentation\n",
    "\n",
    "The approach taken is to classify small tiles of the input image using the DCNN, then estimate the class for the rest of the image pixels using a conditional random field\n",
    "\n",
    "Implements the technique outlined by [Buscombe & Ritchie (2018)](http://www.mdpi.com/2076-3263/8/7/244)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs/dl_tools_semseg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os, time, sys\n",
    "from glob import glob\n",
    "from scipy.misc import imread\n",
    "\n",
    "#numerical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pydensecrf.densecrf as dcrf\n",
    "from pydensecrf.utils import create_pairwise_bilateral, unary_from_labels, unary_from_softmax\n",
    "from numpy.lib.stride_tricks import as_strided as ast\n",
    "import random, string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# suppress divide and invalid warnings\n",
    "np.seterr(divide='ignore')\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as colors\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define subfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for tiling images into windows and loading the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "def norm_shape(shap):\n",
    "   '''\n",
    "   Normalize numpy array shapes so they're always expressed as a tuple,\n",
    "   even for one-dimensional shapes.\n",
    "   '''\n",
    "   try:\n",
    "      i = int(shap)\n",
    "      return (i,)\n",
    "   except TypeError:\n",
    "      # shape was not a number\n",
    "      pass\n",
    "\n",
    "   try:\n",
    "      t = tuple(shap)\n",
    "      return t\n",
    "   except TypeError:\n",
    "      # shape was not iterable\n",
    "      pass\n",
    "\n",
    "   raise TypeError('shape must be an int, or a tuple of ints')\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Return a sliding window over a in any number of dimensions\n",
    "# version with no memory mapping\n",
    "def sliding_window(a,ws,ss = None,flatten = True):\n",
    "    '''\n",
    "    Return a sliding window over a in any number of dimensions\n",
    "    '''\n",
    "    if None is ss:\n",
    "        # ss was not provided. the windows will not overlap in any direction.\n",
    "        ss = ws\n",
    "    ws = norm_shape(ws)\n",
    "    ss = norm_shape(ss)\n",
    "    # convert ws, ss, and a.shape to numpy arrays\n",
    "    ws = np.array(ws)\n",
    "    ss = np.array(ss)\n",
    "    shap = np.array(a.shape)\n",
    "    # ensure that ws, ss, and a.shape all have the same number of dimensions\n",
    "    ls = [len(shap),len(ws),len(ss)]\n",
    "    if 1 != len(set(ls)):\n",
    "        raise ValueError(\\\n",
    "        'a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n",
    "\n",
    "    # ensure that ws is smaller than a in every dimension\n",
    "    if np.any(ws > shap):\n",
    "        raise ValueError(\\\n",
    "        'ws cannot be larger than a in any dimension.\\\n",
    " a.shape was %s and ws was %s' % (str(a.shape),str(ws)))\n",
    "    # how many slices will there be in each dimension?\n",
    "    newshape = norm_shape(((shap - ws) // ss) + 1)\n",
    "    # the shape of the strided array will be the number of slices in each dimension\n",
    "    # plus the shape of the window (tuple addition)\n",
    "    newshape += norm_shape(ws)\n",
    "    # the strides tuple will be the array's strides multiplied by step size, plus\n",
    "    # the array's strides (tuple addition)\n",
    "    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n",
    "    a = ast(a,shape = newshape,strides = newstrides)\n",
    "    if not flatten:\n",
    "        return a\n",
    "    # Collapse strided so that it has one more dimension than the window.  I.e.,\n",
    "    # the new array is a flat list of slices.\n",
    "    meat = len(ws) if ws.shape else 0\n",
    "    firstdim = (np.product(newshape[:-meat]),) if ws.shape else ()\n",
    "    dim = firstdim + (newshape[-meat:])\n",
    "    # remove any dimensions with size 1\n",
    "    #dim = filter(lambda i : i != 1,dim)\n",
    "\n",
    "    return a.reshape(dim), newshape\n",
    "\n",
    "# =========================================================\n",
    "def load_graph(model_file):\n",
    "  graph = tf.Graph()\n",
    "  graph_def = tf.GraphDef()\n",
    "\n",
    "  with open(model_file, \"rb\") as f:\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  with graph.as_default():\n",
    "    tf.import_graph_def(graph_def)\n",
    "\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function that actually does the prediction\n",
    "\n",
    "1. The image is fed into the classifier\n",
    "2. The probability of each class is predicted (```results```)\n",
    "3. They are reordered from best to worst\n",
    "4. The function returns\n",
    "    * the numeric code of class with the highest probability\n",
    "    * the probability of that prediction\n",
    "    * a vector of probabilities in order from highest to lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "def getCP(tmp, graph):\n",
    "\n",
    "   input_name = \"import/Placeholder\" #input\" \n",
    "   output_name = \"import/final_result\" \n",
    "\n",
    "   input_operation = graph.get_operation_by_name(input_name);\n",
    "   output_operation = graph.get_operation_by_name(output_name);\n",
    "\n",
    "   with tf.Session(graph=graph) as sess:\n",
    "      results = sess.run(output_operation.outputs[0],\n",
    "                      {input_operation.outputs[0]: np.expand_dims(tmp, axis=0)})\n",
    "   results = np.squeeze(results)\n",
    "\n",
    "   # Sort to show labels of first prediction in order of confidence\n",
    "   top_k = results.argsort()[-len(results):][::-1]\n",
    "\n",
    "   return top_k[0], results[top_k[0]] ##, results[top_k] #, np.std(tmp[:,:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is normalized so it is less sensitive to brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "def norm_im(img): \n",
    "   input_mean = 0 #128\n",
    "   input_std = 255 #128\n",
    "\n",
    "   input_name = \"file_reader\"\n",
    "   output_name = \"normalized\"\n",
    "   nx, ny, nz = np.shape(img)\n",
    "\n",
    "   theta = np.std(img).astype('int')\n",
    "   float_caster = tf.cast(img, tf.float32)\n",
    "   \n",
    "   dims_expander = tf.expand_dims(float_caster, 0);\n",
    "   normalized = tf.divide(tf.subtract(dims_expander, [input_mean]), [input_std])\n",
    "   sess = tf.Session()\n",
    "   return np.squeeze(sess.run(normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "def getCRF(image, Lc, theta, n_iter, label_lines, compat_spat=12, compat_col=40, scale=5, prob=0.5):\n",
    "\n",
    "      H = image.shape[0]\n",
    "      W = image.shape[1]\n",
    "\n",
    "      d = dcrf.DenseCRF2D(H, W, len(label_lines)+1)\n",
    "      U = unary_from_labels(Lc.astype('int'), len(label_lines)+1, gt_prob= prob)\n",
    "\n",
    "      d.setUnaryEnergy(U)\n",
    "\n",
    "      del U\n",
    "\n",
    "      # This potential penalizes small pieces of segmentation that are\n",
    "      # spatially isolated -- enforces more spatially consistent segmentations\n",
    "      # This adds the color-independent term, features are the locations only.\n",
    "      # sxy = The scaling factors per dimension.\n",
    "      d.addPairwiseGaussian(sxy=(theta,theta), compat=compat_spat, kernel=dcrf.DIAG_KERNEL, #compat=6\n",
    "                      normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "\n",
    "      # sdims = The scaling factors per dimension.\n",
    "      # schan = The scaling factors per channel in the image.\n",
    "      # This creates the color-dependent features and then add them to the CRF\n",
    "      feats = create_pairwise_bilateral(sdims=(theta, theta), schan=(scale, scale, scale), #11,11,11\n",
    "                                  img=image, chdim=2)\n",
    "\n",
    "      del image\n",
    "\n",
    "      d.addPairwiseEnergy(feats, compat=compat_col, #20\n",
    "                    kernel=dcrf.DIAG_KERNEL,\n",
    "                    normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
    "      del feats\n",
    "\n",
    "      Q = d.inference(n_iter)\n",
    "\n",
    "      return np.argmax(Q, axis=0).reshape((H, W)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic segmentation of Lake Ontario shoreline imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels and tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'ontario_labels.txt'  \n",
    "classifier_file = 'ontario_test_mobilenetv2_224_1000_0.01.pb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a file that contains colors for each class in the labels file.\n",
    "\n",
    "First, let's remind ourselves of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ontario_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program wants as input a set of colors for each label. This is for plotting purposes\n",
    "\n",
    "It really helps to colorize labels in an intuitive way, e.g. water = blue. \n",
    "\n",
    "Let's define colors in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['r', 'w', 'm', 'g', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ontario_colors.txt\n",
    "with open('ontario_colors.txt', 'a') as the_file:\n",
    "    for c in cols:\n",
    "        the_file.write(c+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_path = 'ontario_colors.txt'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $colors_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use html codes for more custom colors\n",
    "\n",
    "https://www.w3schools.com/colors/colors_picker.asp\n",
    "\n",
    "https://www.w3schools.com/colors/colors_names.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm $colors_path\n",
    "cols = ['#A52A2A', '#FFD700', '#808000', '#00FF7F', '#4682B4']\n",
    "\n",
    "with open('ontario_colors.txt', 'a') as the_file:\n",
    "    for c in cols:\n",
    "        the_file.write(c+'\\n')\n",
    "        \n",
    "!cat $colors_path        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part loads the labels and colors and creates a colormap for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads label file, strips off carriage return\n",
    "labels = [line.rstrip() for line \n",
    "                in tf.gfile.GFile(labels_path)]\n",
    "\n",
    "code= {}\n",
    "for label in labels:\n",
    "   code[label] = [i for i, x in enumerate([x.startswith(label) for x in labels]) if x].pop()\n",
    "\n",
    "with open(colors_path) as f: #'labels.txt') as f:\n",
    "   cols = f.readlines()\n",
    "cmap1 = [x.strip() for x in cols] \n",
    " \n",
    "classes = dict(zip(labels, cmap1))\n",
    "\n",
    "cmap1 = colors.ListedColormap(cmap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file (S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ontario dataset sits on S3. Let's load an image in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "testimage = fs.ls('esipfed/cdi-workshop/semseg_data/ontario/test/')[0]\n",
    "with fs.open(testimage, 'rb') as f:\n",
    "    img = imread(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-defined settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = 224 ## the size of the tile (corresponds to the size used to train the model)\n",
    "winprop = 1.0 # the proportion of each tile to use as input to the CRF\n",
    "prob_thres = 0.5 # threshold probability. Below this, DCNN classifications are ignored\n",
    "n_iter = 30 # number of iterations in CRF model\n",
    "compat_col = 100 # compatibility function (color)\n",
    "theta = 60 #std deviation terms (color and spatial)\n",
    "scale = 1 # weight term in CRF\n",
    "decim = 2 #8 # 1/proportion of image to use in DCNN\n",
    "fct =  0.25 # scale of image to use. If <1, image will be downsclaed to that fraction\n",
    "compat_spat = 5 #compatability function (spatial)\n",
    "prob = 0.5 # the likelihood of the CRF unary potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore these settings a little"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ```tile```: you should use a tile size that is the same as the tile size you used to train the model you intend to run\n",
    "* ```winprop```: this is usually 1.0 unless you have a specific reason to assign the classification to less than the portion of the image represented by a tile\n",
    "* ```prob_thres```: the higher this number, the fewer DCNN classifications are used\n",
    "* ```prob```: the probability of the DCNN prediction. This is used by the CRF but doesn't usually matter to the final result that much\n",
    "* ```n_iter```: the higher this number, the longer it takes for the CRF portion of the procedure to run, but larger numbers of iterations might result in more refined/accurate predictions\n",
    "* ```compat_col```: label compatibilities for the colour-dependent term in the CRF model. Controls the minimum distance (in colorspace) between two nodes in the model that are assigned different labels  \n",
    "* ```compat_spat```: label compatibilities for the spatial-dependent term in the CRF model. Controls the minimum distance (in space) between two nodes in the model that are assigned different labels  \n",
    "* ```scale```: a kernel weighting term in the CRF. Always 1 unless you a have a specific reason to weight color and space differently\n",
    "* ```theta```:  controls the degree of allowable similarity in image intensity between CRF graph nodes. Larger = more dissimilarity is tolerated. Relatively large means image features with relatively large differences in intensity are assigned the same label\n",
    "* ```decim```: typically, you don't need to predict the class of every tile in the image. This number controls the decimation. 2 = use every second tile, 4 = every 4th tile, etc\n",
    "* ```fct```: to save time during the CRF phase, the image is downscaled (the rescaled back up to the original size). This number controls the scaling: 0.5 means half the original size, 0.25 means a quarter, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've broken this down into stages so it's easier to follow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Normalize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxo, nyo, nzo = np.shape(img)\n",
    "result = norm_im(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Pad image using mirroring and create tiles by using sliding window. The image padding helps us deal with image boundary effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pad image so it is divisible by N windows with no remainder \n",
    "result = np.vstack((np.hstack((result,np.fliplr(result))), np.flipud(np.hstack((result,np.fliplr(result))))))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(121); plt.imshow(result); plt.axis('off'); plt.title('Mirrored')\n",
    "\n",
    "result = result[:nxo+np.mod(nxo,tile),:nyo+np.mod(nyo,tile), :]\n",
    "\n",
    "plt.subplot(122); plt.imshow(result); plt.axis('off'); plt.title('Cropped')\n",
    "\n",
    "nx, ny, nz = np.shape(result)\n",
    "\n",
    "gridy, gridx = np.meshgrid(np.arange(ny), np.arange(nx))\n",
    "Zx,_ = sliding_window(gridx, (tile,tile), (tile,tile))\n",
    "Zy,_ = sliding_window(gridy, (tile,tile), (tile,tile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load graph and partition image into tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if decim>1:\n",
    "   Zx = Zx[::decim]\n",
    "   Zy = Zy[::decim]\n",
    "\n",
    "graph = load_graph(classifier_file)\n",
    "\n",
    "w1 = []\n",
    "Z,ind = sliding_window(result, (tile,tile,3), (tile, tile,3))\n",
    "if decim>1:\n",
    "   Z = Z[::decim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of those tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Z[i])\n",
    "    axi.set(xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: For each window, estimate the class. This takes a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Z)):\n",
    "   w1.append(getCP(Z[i], graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Create output arrays, filter out tiles with low probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##C=most likely, P=prob\n",
    "C, P = zip(*w1)\n",
    "\n",
    "C = np.asarray(C)\n",
    "P = np.asarray(P)\n",
    "\n",
    "C = C+1 #add 1 so all labels are >=1\n",
    "\n",
    "## create images with classes and probabilities\n",
    "Lc = np.zeros((nx, ny))\n",
    "Lp = np.zeros((nx, ny))\n",
    "\n",
    "mn = np.int(tile-(tile*winprop)) #tile/2 - tile/4)\n",
    "mx = np.int(tile+(tile*winprop)) #tile/2 + tile/4)\n",
    "\n",
    "for k in range(len(Zx)): \n",
    "   Lc[Zx[k][mn:mx,mn:mx], Zy[k][mn:mx,mn:mx]] = Lc[Zx[k][mn:mx,mn:mx], Zy[k][mn:mx,mn:mx]]+C[k] \n",
    "   Lp[Zx[k][mn:mx,mn:mx], Zy[k][mn:mx,mn:mx]] = Lp[Zx[k][mn:mx,mn:mx], Zy[k][mn:mx,mn:mx]]+P[k] \n",
    "     \n",
    "Lp = Lp[:nxo, :nyo]      \n",
    "Lc = Lc[:nxo, :nyo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: filter out low probabilities (according to ```prob_thres```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lcorig = Lc.copy()\n",
    "Lcorig[Lp < prob_thres] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(Lc)\n",
    "plt.colorbar(shrink=0.25)\n",
    "\n",
    "Lc[np.isnan(Lcorig)] = 0\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(Lc)\n",
    "plt.colorbar(shrink=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: We're going to resize to speed things up a little in the next step (CRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgr = imresize(img, fct)\n",
    "Lcr = np.round(imresize(Lc, fct, interp='nearest')/255 * np.max(Lc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(imgr))\n",
    "print(np.shape(Lcr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Conditional Random Field post-processing. This takes a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CRF ... ')\n",
    "res = getCRF(imgr, Lcr.astype('int'), theta, n_iter, labels, compat_spat, compat_col, scale, prob)\n",
    "\n",
    "del imgr\n",
    "resr = np.round(imresize(res, 1/fct, interp='nearest')/255 * np.max(res))\n",
    "   \n",
    "code1 = np.unique(res)\n",
    "code2 = np.unique(resr)   \n",
    "resrr = np.zeros(np.shape(resr), dtype='int8')\n",
    "for kk in range(len(code1)):\n",
    "   resrr[resr==code2[kk]] = code1[kk]   \n",
    "   \n",
    "del res, resr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,15))\n",
    "fig.subplots_adjust(wspace=0.1)\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.get_yaxis().set_visible(False)\n",
    "\n",
    "im = ax1.imshow(img)\n",
    "plt.title('a) Input', loc='left', fontsize=10)\n",
    "\n",
    "ax1 = fig.add_subplot(132)\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.get_yaxis().set_visible(False)\n",
    "\n",
    "im = ax1.imshow(img)\n",
    "plt.title('b) CNN prediction', loc='left', fontsize=10)\n",
    "im2 = ax1.imshow(Lcorig-1, cmap=cmap1, alpha=0.5, vmin=0, vmax=len(labels))\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\")\n",
    "cb=plt.colorbar(im2, cax=cax)\n",
    "cb.set_ticks(.5+np.arange(len(labels)+1)) \n",
    "cb.ax.set_yticklabels(labels)\n",
    "cb.ax.tick_params(labelsize=6) \n",
    "plt.axis('tight')\n",
    "\n",
    "ax1 = fig.add_subplot(133)\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.get_yaxis().set_visible(False)\n",
    "\n",
    "im = ax1.imshow(img)\n",
    "plt.title('c) CRF prediction', loc='left', fontsize=10)\n",
    "im2 = ax1.imshow(resrr, cmap=cmap1, alpha=0.5, vmin=0, vmax=len(labels))\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\")\n",
    "cb=plt.colorbar(im2, cax=cax)\n",
    "cb.set_ticks(.5+np.arange(len(labels)+1)) \n",
    "cb.ax.set_yticklabels(labels)\n",
    "cb.ax.tick_params(labelsize=6)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't bad considering we only trained our classifier on a 10th of available data (to save time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better prediction usually involves one of the following (in usual order of importance):\n",
    "    \n",
    "1. Training on more data / optimizing training parameters\n",
    "2. Careful selection of tile size for the data set\n",
    "3. Number of tiles to use (```decim``` parameter)\n",
    "4. Othe input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Ontario (and many other) datasets, we have a pretrained classifier, trained on more data. Let's use that one and compare the semantic segmentation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a pretrained model file from Google drive\n",
    "\n",
    "All the data for this workshop is on a google drive located here: https://drive.google.com/drive/folders/1IhStVBhWMKLZUWIprti6zZyOg32-W4Of?usp=sharing\n",
    "\n",
    "\n",
    "First we'll write a function to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a new directory to put our downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = 'gdrive_downloads'\n",
    "import os\n",
    "os.mkdir(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file id for this particular pretrained model is\n",
    "\n",
    "https://drive.google.com/open?id=1lLjayLPILSfEUji-lh_d5lydb4voWzhU\n",
    "    \n",
    "You'll need the part after ```id=```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = '1lLjayLPILSfEUji-lh_d5lydb4voWzhU'\n",
    "\n",
    "classifier_file = destination+os.sep+'ontario224_gdrive.pb'\n",
    "\n",
    "download_file_from_google_drive(file_id, classifier_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier, we can use the ```semseg_cnn_crf.py``` file provided which does the same thing as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./semseg_cnn_crf.py $testimage $classifier_file $labels_path $colors_path $tile $prob_thres $prob $decim $fct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this result compare to the result we generated before, using the model trained on less data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will print an image showing the results, and will also create a .mat file which contains the results of the classification. \n",
    "\n",
    "(yes, the same format as matlab)\n",
    "\n",
    "Let's take a look at the contents of this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "dat = loadmat('A2013218_geotag_ares_224.mat')\n",
    "\n",
    "# dat is a dictionary object\n",
    "dat.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these variables?\n",
    "\n",
    "1. ```sparse```\n",
    "    * the DCNN-derived unary potentials\n",
    "2. ```class```\n",
    "    * the pixelwise label\n",
    "3. ```labels```\n",
    "    * the class names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the pixelwise classification like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "ax1.get_yaxis().set_visible(False)\n",
    "\n",
    "im = ax1.imshow(img)\n",
    "im2 = ax1.imshow(dat['class'], cmap=cmap1, alpha=0.5, vmin=0, vmax=len(labels))\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\")\n",
    "cb=plt.colorbar(im2, cax=cax)\n",
    "cb.set_ticks(.5+np.arange(len(labels)+1)) \n",
    "cb.ax.set_yticklabels(labels)\n",
    "cb.ax.tick_params(labelsize=6)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write out a label image. To do this we need to convert our color codes into RGB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "hexcol = [col.lstrip('#').split('\\n')[0] for col in cols]\n",
    "Label = namedtuple('Label', ['name', 'color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = [tuple(int(h[i:i+2], 16) for i in (0, 2 ,4)) for h in hexcol]\n",
    "\n",
    "label_defs = []\n",
    "for k in range(len(labels)):\n",
    "   label_defs.append(Label(labels[k],(rgb[k][0], rgb[k][1], rgb[k][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_defs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 3-channel array and fill with red, green and blue values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.zeros((np.shape(dat['class'])[0], np.shape(dat['class'])[1], 3), dtype='uint8')\n",
    "for k in np.unique(dat['class']):\n",
    "    out[:,:,0][dat['class']==k] = label_defs[k].color[0]\n",
    "    out[:,:,1][dat['class']==k] = label_defs[k].color[1]\n",
    "    out[:,:,2][dat['class']==k] = label_defs[k].color[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally write the RGB image to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imwrite\n",
    "name, ext = os.path.splitext(testimage)\n",
    "name = name.split(os.sep)[-1] \n",
    "imwrite(name+'_gtFine_color.png', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidying up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm A2013218_geotag_ares_224.mat\n",
    "!rm A2013218_geotag_ares_224.png\n",
    "!rm A2013218_geotag_gtFine_color.png\n",
    "!rm ontario_colors.txt\n",
    "!rm ontario_labels.txt\n",
    "!rm ontario_test_mobilenetv2_224_1000_0.01.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf S3data_tile_224/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to know more about conditional random fields, here's a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "### conditional random fields\n",
    "YouTubeVideo(\"rc3YDj5GiVM\") ##22 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read a local file in (such as one in ```ccr_test```) and carry out a semantic segmentation with the appropriate retrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of [named colors](https://matplotlib.org/examples/color/named_colors.html) within matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ccr_colors.txt\n",
    "!rm ccr_labels.txt\n",
    "!rm ccr_mobilenetv2_224_1000_0.01_ft.pb\n",
    "!rm ccr_test_mobilenetv2_224_1000_0.01.pb\n",
    "!rm -rf gdrive_downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent function in DL-tools is called and is the same as used here, i.e.\n",
    "\n",
    "```python semseg_crf\\semseg_cnn_crf.py testimage classifier_file labels_path colors_path tile prob_thres prob decim fct```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
