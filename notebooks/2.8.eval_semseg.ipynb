{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating semantic classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Written by Dr Daniel Buscombe, Northern Arizona University\n",
    "\n",
    "> Part of a series of notebooks for image recognition and classification using deep convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to evaluate how good your retrained DCNN model is at semantic segmentation (classifying image pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs/dl_tools_eval_semseg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll load some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our usual confusion matrix plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## =========================================================\n",
    "def plot_confusion_matrix2(cm, classes, normalize=False, cmap=plt.cm.Blues, dolabels=True):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm[np.isnan(cm)] = 0\n",
    "\n",
    "    cm = cm[:len(classes),:len(classes)]    \n",
    "        \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmax=1, vmin=0)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    if dolabels==True:\n",
    "       tick_marks = np.arange(len(classes))\n",
    "       plt.xticks(tick_marks, classes, fontsize=8) # rotation=45\n",
    "       plt.yticks(tick_marks, classes, fontsize=8)\n",
    "\n",
    "       plt.ylabel('True label',fontsize=6)\n",
    "       plt.xlabel('Estimated label',fontsize=6)\n",
    "\n",
    "    else:\n",
    "       plt.axis('off')\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if cm[i, j]>0:\n",
    "           plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 fontsize=8,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the google download function we saw in the last exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take some files from the google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = 'gdrive_downloads'\n",
    "import os\n",
    "os.mkdir(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://drive.google.com/open?id=1-NPqL7dPEQ3Q87lvDkeBtKYdA0ryhSaT\n",
    "file_id = '1-NPqL7dPEQ3Q87lvDkeBtKYdA0ryhSaT'\n",
    "classifier_file = destination+os.sep+'monterey96_gdrive.pb'\n",
    "\n",
    "download_file_from_google_drive(file_id, classifier_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ground truth class file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://drive.google.com/open?id=1MXaPbb-nPBg4y2LmKLJ0Q-jmQY91VVl0\n",
    "file_id = '1MXaPbb-nPBg4y2LmKLJ0Q-jmQY91VVl0'\n",
    "\n",
    "gt_matfile = destination+os.sep+'monterey_example_gt.mat'\n",
    "download_file_from_google_drive(file_id, gt_matfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://drive.google.com/open?id=1G4deh3kOUNtPFsV0yWSngsM4TVth1-6C\n",
    "file_id = '1G4deh3kOUNtPFsV0yWSngsM4TVth1-6C'\n",
    "\n",
    "testimage = destination+os.sep+'monterey_example.jpg'\n",
    "download_file_from_google_drive(file_id, testimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://drive.google.com/open?id=1qSDLER1IuFGG35PZJyPI05tqh2YQOjkV\n",
    "file_id = '1qSDLER1IuFGG35PZJyPI05tqh2YQOjkV'\n",
    "\n",
    "labels_path = destination+os.sep+'monterey_labels.txt'\n",
    "download_file_from_google_drive(file_id, labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load colors file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://drive.google.com/open?id=128INeq_qJ6y9p7WtHnVk9YNrnSL5hSxS\n",
    "file_id = '128INeq_qJ6y9p7WtHnVk9YNrnSL5hSxS'\n",
    "\n",
    "colors_path = destination+os.sep+'monterey_colors.txt'\n",
    "download_file_from_google_drive(file_id, colors_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look in the ```gdrive_downloads``` folder to make sure they are all in there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pixelwise prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = 96 ## the size of the tile (corresponds to the size used to train the model)\n",
    "winprop = 1.0 # the proportion of each tile to use as input to the CRF\n",
    "prob_thres = 0.5 # threshold probability. Below this, DCNN classifications are ignored\n",
    "n_iter = 20 # number of iterations in CRF model\n",
    "compat_col = 100 # compatibility function (color)\n",
    "theta = 60 #std deviation terms (color and spatial)\n",
    "scale = 1 # weight term in CRF\n",
    "decim = 16 # 1/proportion of image to use in DCNN\n",
    "fct =  0.125 # scale of image to use. If <1, image will be downsclaed to that fraction\n",
    "compat_spat = 5 #compatability function (spatial)\n",
    "prob = 0.5 # the likelihood of the CRF unary potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pixelwise classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./semseg_cnn_crf.py $testimage $classifier_file $labels_path $colors_path $tile $prob_thres $prob $decim $fct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate classification pixel-by-pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's compare pixel by pixel\n",
    "\n",
    "First, we'll load the 'ground truth' label image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = loadmat(gt_matfile)['class']\n",
    "print(np.shape(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the estimate that we just generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_matfile = 'monterey_example_ares_96.mat'\n",
    "a = loadmat(est_matfile)['class']\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alabs = loadmat(est_matfile)['labels']\n",
    "clabs = loadmat(gt_matfile)['labels']\n",
    "\n",
    "alabs = [label.replace(' ','') for label in alabs]\n",
    "clabs = [label.replace(' ','') for label in clabs]\n",
    "cind = [clabs.index(x) for x in alabs]\n",
    "aind = [alabs.index(x) for x in alabs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code deals with the eventuality that the ground truth and estimate have different order of numeric codes\n",
    "Cmaster = np.zeros((len(alabs), len(alabs)))\n",
    "c2 = c.copy()\n",
    "for kk in range(len(aind)):\n",
    "    if cind[kk] != aind[kk]:\n",
    "        c2[c==cind[kk]] = aind[kk] \n",
    "del c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare visually\n",
    "\n",
    "To do this we'll load in the image and colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread(testimage)\n",
    "\n",
    "with open(colors_path) as f: #'labels.txt') as f:\n",
    "   cols = f.readlines()\n",
    "cmap1 = [x.strip() for x in cols] \n",
    "cmap1 = colors.ListedColormap(cmap1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.imshow(a, cmap=cmap1, alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.title('Ground truth')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img)\n",
    "plt.imshow(c2, cmap=cmap1, alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.title('Estimate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = np.shape(a)\n",
    "print('Accuracy is '+str(np.sum((a==c2).flatten()/(nx*ny)))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at things is to see the difference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(a - c2, cmap='bwr')\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(a == c2, cmap=plt.cm.binary_r)\n",
    "plt.axis('off')\n",
    "plt.title('Proportion correct:' + str(np.sum((a==c2).flatten()/(nx*ny)))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a confusion matrix to look at class-by-class comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(alabs)\n",
    "cm = np.zeros((n,n))\n",
    "for amat, pmat in zip(a.flatten(), c2.flatten()):\n",
    "    cm[amat][pmat] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "_ = plot_confusion_matrix2(cm, classes=alabs, normalize=True, cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things to remember:\n",
    "1. there is error in the ground truth label\n",
    "2. while the overall pattern is qualitatively very similar, large numbers of individual pixels can still be misclassified\n",
    "\n",
    "These effects are exacerbated by \n",
    "* large numbers of classes (especially very similar classes)\n",
    "* large spatial heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```precision_recall_fscore_support``` function (part of scikit-learn) doesn't do well when there aren't examples of pixels for each class\n",
    "\n",
    "It basically includes zeroes in the average. Let's look at the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "e = precision_recall_fscore_support(a.flatten(), c2.flatten())\n",
    "\n",
    "p = np.mean(e[0])\n",
    "r = np.mean(e[1])\n",
    "f = np.mean(e[2])\n",
    "print('mean precision: %f' %(p))\n",
    "print('mean recall: %f' %(r))\n",
    "print('mean f-score: %f' %(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write our own function that computes all three, ignoring classes with no support\n",
    "\n",
    "Let's remind ourselves of the formulae for precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P=  \\frac{TP}{(TP+FP)}$\n",
    "\n",
    "$R=  \\frac{TP}{(TP+FN)}$\n",
    "\n",
    "$F=2\\times \\frac{(P \\times R)}{(P+R)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(cm):\n",
    "    \n",
    "    m, n = np.shape(cm)\n",
    "    \n",
    "    TP = []\n",
    "    for x in range(n):\n",
    "        TP.append(cm[x, x])\n",
    "    \n",
    "    FP = []\n",
    "    for x in range(n):\n",
    "        FP.append(sum(cm[:, x])-cm[x, x])\n",
    "\n",
    "    FN = []\n",
    "    for x in range(n):\n",
    "        FN.append(sum(cm[x, :], 2)-cm[x, x])    \n",
    "        \n",
    "    tp = np.asarray(TP)\n",
    "    fp = np.asarray(FP)\n",
    "    fn = np.asarray(FN)    \n",
    "        \n",
    "    p = tp/(tp+fp)\n",
    "    p[p==0] = np.nan\n",
    "\n",
    "    r = tp/(tp+fn)\n",
    "    r[r==0] = np.nan    \n",
    "\n",
    "    p = np.nanmean(p)\n",
    "    r = np.nanmean(r)\n",
    "    \n",
    "    f = 2*((p*r)/(p+r))\n",
    "    \n",
    "    return p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f = get_stats(cm)\n",
    "print('mean precision: %f' %(p))\n",
    "print('mean recall: %f' %(r))\n",
    "print('mean f-score: %f' %(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big difference. Be mindful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf gdrive_downloads\n",
    "!rm monterey_example_ares_96.mat\n",
    "!rm monterey_example_ares_96.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent function in DL-tools is called and is the same as used here, i.e.\n",
    "\n",
    "```python eval_semseg\\test_pixels.py```\n",
    "\n",
    "You are asked to select a directory that contains the ground truth and estimates class files (*.mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
