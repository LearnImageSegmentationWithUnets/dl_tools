{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating image tile classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Written by Dr Daniel Buscombe, Northern Arizona University\n",
    "\n",
    "> Part of a series of notebooks for image recognition and classification using deep convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to evaluate how good your retrained DCNN model is at image recognition (classifying whole images or image tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs/dl_tools_eval_tile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard metrics of precision, P, recall, R, accuracy, A, and F1 score, F, are used to assess classification of image regions and pixels. \n",
    "\n",
    "Where TP, TN, FP, and FN are, respectively, the frequencies of true positives, true negatives, false positives, and false negatives: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P=  \\frac{TP}{(TP+FP)}$\n",
    "\n",
    "$R=  \\frac{TP}{(TP+FN)}$\n",
    "\n",
    "$A=  \\frac{(TP+TN)}{(TP+TN+FP+FN)}$\n",
    "\n",
    "$F=2\\times \\frac{(P \\times R)}{(P+R)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positives are image regions/pixels correctly classified as belonging to a certain class by the model, while true negatives are correctly classified as not belonging to a certain class. \n",
    "\n",
    "False negatives are regions/pixels incorrectly classified as not belonging to a certain class, and false positives are those regions/pixels incorrectly classified as belonging to a certain class. \n",
    "\n",
    "Precision and recall are useful where the number of observations belonging to one class is significantly lower than those belonging to the other classes. \n",
    "\n",
    "Recall is a measure of the ability to detect the occurrence of a class, which is a given landform, land use or land cover.\n",
    " \n",
    "These metrics are therefore used in evaluation of pixelwise segmentations, where the number of pixels corresponding to each class vary considerably. \n",
    "\n",
    "The F1 score is an equal weighting of the recall and precision and quantifies how well the model performs in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from glob import glob\n",
    "from imageio import imread\n",
    "import itertools\n",
    "\n",
    "#numerical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#supress tf warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# suppress divide and invalid warnings\n",
    "np.seterr(divide='ignore')\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting model and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = 'ccr_labels.txt'\n",
    "classifier_file = 'ccr_test_mobilenetv2_224_1000_0.01.pb'\n",
    "\n",
    "## number of tiles to evaluate\n",
    "numero = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads label file, strips off carriage return\n",
    "labels = [line.rstrip() for line \n",
    "             in tf.gfile.GFile(class_file)]\n",
    "code= {}\n",
    "for label in labels:\n",
    "    code[label] = [i for i, x in enumerate([x.startswith(label) for x in labels]) if x].pop()\n",
    "\n",
    "code    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the folder of image tiles that we want to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = 'ccr_test/tile_224'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are our functions to load the computational graph (.pb file), normalizing images, and carrying out the classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "def load_graph(model_file):\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# =========================================================\n",
    "def getCP(tmp, graph):  \n",
    "    #graph = load_graph(classifier_file)\n",
    "\n",
    "    input_name = \"import/Placeholder\" ##+ input_layer\n",
    "    output_name = \"import/final_result\" ##+ output_layer\n",
    "    input_operation = graph.get_operation_by_name(input_name);\n",
    "    output_operation = graph.get_operation_by_name(output_name);\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        results = sess.run(output_operation.outputs[0],\n",
    "                      {input_operation.outputs[0]: np.expand_dims(tmp, axis=0)})\n",
    "    results = np.squeeze(results)\n",
    "\n",
    "    # Sort to show labels of first prediction in order of confidence\n",
    "    top_k = results.argsort()[-len(results):][::-1]\n",
    "\n",
    "    return top_k[0], results[top_k[0]], results[top_k] #, np.std(tmp[:,:,0])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "def norm_im(image_path):\n",
    "    input_mean = 0 #128\n",
    "    input_std = 255 #128\n",
    "\n",
    "    input_name = \"file_reader\"\n",
    "    output_name = \"normalized\"\n",
    "    img = imread(image_path)\n",
    "    nx, ny, nz = np.shape(img)\n",
    "\n",
    "    theta = np.std(img).astype('int')\n",
    "\n",
    "    file_reader = tf.read_file(image_path, input_name)\n",
    "    image_reader = tf.image.decode_jpeg(file_reader, channels = 3,\n",
    "                                        name='jpeg_reader')\n",
    "    float_caster = tf.cast(image_reader, tf.float32)\n",
    "\n",
    "    dims_expander = tf.expand_dims(float_caster, 0);\n",
    "    normalized = tf.divide(tf.subtract(dims_expander, [input_mean]), [input_std])\n",
    "    sess = tf.Session()\n",
    "    return np.squeeze(sess.run(normalized))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the wrapper function that:\n",
    "    \n",
    "1. loads the graph\n",
    "2. finds images in a category\n",
    "3. normalizes each image\n",
    "4. gets the classification for each image\n",
    "5. computes the precision, recall and F1 scores\n",
    "6. computes the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tiles(label, direc, numero, classifier_file, x, n):\n",
    "    #for label in labels:\n",
    "    graph = load_graph(classifier_file)\n",
    "\n",
    "    print(label)\n",
    "    infiles = glob(direc+os.sep+label+os.sep+'*.jpg')[:numero]\n",
    "\n",
    "    Z = []\n",
    "    for image_path in infiles:\n",
    "        Z.append(norm_im(image_path))\n",
    "\n",
    "    w1 = []\n",
    "    for i in range(len(Z)):\n",
    "        w1.append(getCP(Z[i], graph))\n",
    "\n",
    "    C, P, _ = zip(*w1) ##,S\n",
    "    del w1, Z\n",
    "\n",
    "    C = np.asarray(C)\n",
    "    P = np.asarray(P)\n",
    "\n",
    "    e = precision_recall_fscore_support(np.ones(len(C))*x, C)\n",
    "\n",
    "    cm = np.zeros((n,n))\n",
    "    for a, p in zip(np.ones(len(C), dtype='int')*x, C):\n",
    "        cm[a][p] += 1\n",
    "\n",
    "    cm = cm[x,:]\n",
    "\n",
    "    p = np.max(e[0])\n",
    "    r = np.max(e[1])\n",
    "    f = np.max(e[2])\n",
    "    a = np.sum([c==x for c in C])/len(C)\n",
    "    #print(label+' accuracy %f' % (a))\n",
    "    #print('precision %f' % (p) )\n",
    "    #print('recall %f' % (r) )\n",
    "    #print('f score %f' % (f) )\n",
    "    #print('mean prob. %f' % (np.mean(P)) )\n",
    "    return [a,p,r,f, np.mean(P)], cm ##C,P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating tile classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we can run that function on each image category, in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=[]\n",
    "for label in labels:\n",
    "    w.append(eval_tiles(label, direc, numero, classifier_file, code[label], len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we extract the skill metrics vector (E) and confusion matrix (CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, CM = zip(*w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse out the three skill metrics and print to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.asarray(E)[:,0] \n",
    "f= np.asarray(E)[:,3] \n",
    "pr= np.asarray(E)[:,4] \n",
    "\n",
    "print('mean accuracy. %f' % (np.mean(a)) )\n",
    "print('mean Fscore. %f' % (np.mean(f)) )\n",
    "print('mean prob. %f' % (np.mean(pr)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ‘confusion matrix’, which is the matrix of normalized correspondences between true and estimated labels, is a convenient way to visualize model skill. \n",
    "\n",
    "A perfect correspondence between true and estimated labels is scored 1.0 along the diagonal elements of the matrix.\n",
    "\n",
    "Misclassiﬁcations are readily identiﬁed as off-diagonal elements. Systematic misclassiﬁcations are recognized as off-diagonal elements with large magnitudes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## =========================================================\n",
    "def plot_confusion_matrix2(cm, classes, normalize=False, cmap=plt.cm.Blues, dolabels=True):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm[np.isnan(cm)] = 0\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap, vmax=1, vmin=0)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    if dolabels==True:\n",
    "       tick_marks = np.arange(len(classes))\n",
    "       plt.xticks(tick_marks, classes, fontsize=5) # rotation=45\n",
    "       plt.yticks(tick_marks, classes, fontsize=5)\n",
    "\n",
    "       #plt.ylabel('True label',fontsize=6)\n",
    "       #plt.xlabel('Estimated label',fontsize=6)\n",
    "\n",
    "    else:\n",
    "       plt.axis('off')\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if cm[i, j]>0:\n",
    "           plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 fontsize=5,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = np.asarray(CM)\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax1 = fig.add_subplot(221)\n",
    "plot_confusion_matrix2(CM, classes=labels, normalize=True, cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Ontario classifier (dataset on S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get a list of the test files and make tiles out of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "S3direc='esipfed/cdi-workshop/semseg_data/ontario/test'\n",
    "print(S3direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in fs.ls(S3direc) if f.endswith('.JPG')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tilesize = 224\n",
    "threshold = 0.9\n",
    "prop_keep=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./retile_fromS3.py $S3direc $tilesize $threshold $prop_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specifiy the labels and model files and the directory of test tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = 'ontario_labels.txt'\n",
    "classifier_file = 'ontario_test_mobilenetv2_224_1000_0.01.pb'\n",
    "direc = 'S3data_tile_224/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ```code``` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads label file, strips off carriage return\n",
    "labels = [line.rstrip() for line \n",
    "             in tf.gfile.GFile(class_file)]\n",
    "code= {}\n",
    "for label in labels:\n",
    "    code[label] = [i for i, x in enumerate([x.startswith(label) for x in labels]) if x].pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we can run that function on each image category, in a loop, parse the data and print to screen, as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=[]\n",
    "for label in labels:\n",
    "    w.append(eval_tiles(label, direc, numero, classifier_file, code[label], len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, CM = zip(*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.asarray(E)[:,0] \n",
    "f= np.asarray(E)[:,3] \n",
    "pr= np.asarray(E)[:,4] \n",
    "\n",
    "print('mean accuracy. %f' % (np.mean(a)) )\n",
    "print('mean Fscore. %f' % (np.mean(f)) )\n",
    "print('mean prob. %f' % (np.mean(pr)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating UCMerced classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify label and class file and directory (to be created and populated with imagery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = 'merced_labels.txt'\n",
    "classifier_file = 'merced_test_mobilenetv2_224_2000_0.01.pb'\n",
    "direc = 'UCmerced_tile_224/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll read in image file paths into a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "cats = fs.ls('cdi-workshop/imrecog_data/UCMerced_LandUse/Images/train')\n",
    "for cat in cats:\n",
    "    images.append(fs.ls('cdi-workshop/imrecog_data/UCMerced_LandUse/Images/train/'+cat.split(os.sep)[-1])[:numero])\n",
    "len(images)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(direc)\n",
    "for cat in cats:\n",
    "    os.mkdir(direc+os.sep+cat.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in libraries to write out images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imwrite\n",
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in each file and write out to appropriate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for cat in cats:\n",
    "    ##print(\"working on %s\" % cat)\n",
    "    for image in images[counter]:\n",
    "        with fs.open(image, 'rb') as fim:\n",
    "            imwrite(direc+os.sep+cat.split('/')[-1]+os.sep+image.split(os.sep)[-1], imresize(imread(fim), (224, 224)))\n",
    "    counter +=1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads label file, strips off carriage return\n",
    "labels = [line.rstrip() for line \n",
    "             in tf.gfile.GFile(class_file)]\n",
    "code= {}\n",
    "for label in labels:\n",
    "    code[label] = [i for i, x in enumerate([x.startswith(label) for x in labels]) if x].pop()\n",
    "code    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the same way as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=[]\n",
    "for label in labels:\n",
    "    w.append(eval_tiles(label, direc, numero, classifier_file, code[label], len(labels)))\n",
    "\n",
    "E, CM = zip(*w)\n",
    "\n",
    "a=np.asarray(E)[:,0] \n",
    "f= np.asarray(E)[:,3] \n",
    "pr= np.asarray(E)[:,4] \n",
    "\n",
    "print('mean accuracy. %f' % (np.mean(a)) )\n",
    "print('mean Fscore. %f' % (np.mean(f)) )\n",
    "print('mean prob. %f' % (np.mean(pr)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't want to keep the image tiles, labels and models, do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $direc \n",
    "!rm merced_labels.txt\n",
    "!rm merced_test_mobilenetv2_224_2000_0.01.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ccr_test/tile_224/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we will be using the CCR and Ontario labels and models again in the next lesson, so don't delete those just yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent function in DL-tools is called and is the same as used here, i.e.\n",
    "\n",
    "```python eval_imrecog\\test_class_tiles.py -n 100```\n",
    "\n",
    "where flag ```n``` indicates the number of tiles, randomly selected, to evaluate\n",
    "\n",
    "You are first asked to select a directory that contains the image tiles. Then you are asked to pick a labels file. Finally, you are asked to pick a classifier (.pb model file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
